<TITLE>ARM assembly programming performance issues</TITLE><H1>ARM assembly programming performance issues</H1><HR><H2>About this recipe</H2>This recipe outlines many performance related issues which the ARM Assembly Language Programmer should be aware of.  Many of these issues are dealt with more fully elsewhere in the Cookbook, but are mentioned here for completeness.  This recipe is also useful background for C programmers using <B>armcc</B>, as some of these issues can also apply to programming in C.<P><H2>Performance issues</H2>Not all of the issues in this recipe apply to every ARM processor based system.  However, unless otherwise stated, all issues relate to processors based on ARM6 and ARM7, with or without cache and/or write buffer.<P><H3>LDM / STM</H3>Use LDM and STM instead of a sequence of LDR or STR instructions wherever possible.  This provides several benefits:<P><UL><LI>The code is smaller (and thus will cache better on an ARM processor with a cache);<P><LI>An instruction fetch cycle and a register copy back cycle is saved for each LDR or STR eliminated;<P><LI>On an uncached ARM processor (for LDM) or an unbuffered ARM processor (for STM), non-sequential memory cycles can be turned into faster memory sequential cycles.</UL>For a more detailed discussion of LDM and STM  see <A HREF="./1acbd.html#XREF26996">Flexibility of load and store multiple</A>.<P><H3>Conditional execution</H3>In many situations, branches around short pieces of code can be avoided by using conditionally executed instructions.  This reduces the size of code and may avoid a pipeline break.<P>For a more detailed explanation of the Conditional Execution of ARM Instructions see <A HREF="./1acbb.html#XREF38287">Making the most of conditional execution</A>.<P><H3>Using the Barrel Shifter</H3>Combining shift operations with other operations can significantly increase the code density (and thus performance) of much ARM code.  An explanation of how to use the ARM's barrel shifter is given in <A HREF="./1acbc.html#XREF32971">Using the Barrel Shifter</A>.<P>Many other recipes also demonstrate good use of the barrel shifter.<P><H3>Addressing modes</H3>The ARM instruction set provides a useful selection of addressing modes, which can often be used to improve the performance of code. eg. using LDR or STR pre- or post-indexed with a non zero offset increments the base register and performs the data transfer.  For full details of the addressing modes available refer to the appropriate ARM datasheet.<P><H3>Multiplication</H3>Be aware of the time taken by the ARM multiply and multiply accumulate instructions.  Making the best use of the multiplier is discussed in <A HREF="./2acbe.html#XREF40554">ARM6 multiplier performance issues</A>.<P>When multiplying by a constant value note that using the multiply instruction is often not the optimal solution.  The issues involved are discussed in <A HREF="./2acbb.html#XREF38728">Multiply by a constant</A>.<P><H3>Optimise register usage</H3>Examine your code and see if registers can be reused for another value during parts of a long calculation which uses many registers.  Doing this will reduce the amount of 'register spillage', ie. the number of times a value has to be reloaded or an intermediate value saved and then reloaded.<P>Notice that because data processing is cheap (much can be achieved in a single instruction), keeping a calculated result in a register for use some considerable time later may be less efficient than recalculating it when it is next needed.  This is because it may allow the register so freed to be used for another purpose in the meantime, thus reducing the amount of register spillage. <P>For example code which takes great care to optimise register usage see some of the examples in <A HREF="./2acbe.html#XREF29872">Digital signal processing on the ARM</A>.<P><A NAME="XREF13443"><H3>Loop unrolling</A></H3>Loop unrolling involves using more than one copy of the inner loop of an algorithm.  The following benefits may be gained by loop unrolling:<P><UL><LI>the branch back to the beginning of the loop is executed less frequently;<P><LI>it may be possible to combine some of one iteration with some of the next iteration, and thereby significantly reduce the cost of each iteration.<P><LI>A common case of this is combining LDR or STR instructions from two or more iterations into single LDM or STM instructions.  This reduces code size, the number of instruction fetches, and in the case of LDM, the number of register writeback cycles.</UL>As an example to illustrate the issues involved in loop unrolling, let us consider calculating the following over an array: x[i] = y[i] - y[i+1].  Below is a code fragment which performs this:<P>    <CODE><PRE>    LDR    R2, [R0]                                 ; Preload y[i]Loop    LDR    R3, [R0, #4]!!                           ; Load y[i+1]    SUB    R2, R2, R3                               ; x[i] = y[i] - y[i+1]    STR    R2, [R1], #4                             ; Store x[i]    MOV    R2, R3                                   ; y[i+1] is the next y[i]    CMP    R0, R4                                   ; Finished ?    BLT    Loop  </CODE></PRE>Let us consider the number of execution cycles this will take on an ARM6 based processor.  IF stands for Instruction Fetch, WB stands for Register Write Back, R stands for Read, and W stands for Write.<P>The loop will execute in the following cycles: 6 IF, 1 R, 1 WB, 1 W, and the branch costs an additional 2 IF cycles.  Therefore the total cycle count for processing a 100 element x[] array is:<P><CODE><PRE>799 IF (198 caused by branching), 101 R, 101 WB, 100 W (1198 cycles)Code size: 7 instructions</CODE></PRE>Now consider the effects of unrolling this loop:<P><UL><LI>Branch overhead cycles<P><LI>In the above example there are 198 IF's caused by branching.  Unrolling the loop can clearly reduce this, and the table below shows how progressively unrolling the loop gives reducing returns for the increase in code size:<PRE>--------------------------------------------------------Times       |IF's caused by      |IF saving             Unrolled    |Branching           |                      --------------------------------------------------------2           |98                  |100                   --------------------------------------------------------3           |66                  |134                   --------------------------------------------------------4           |48                  |150                   --------------------------------------------------------5           |38                  |160                   --------------------------------------------------------10          |18                  |180                   --------------------------------------------------------100         |0                   |198                   --------------------------------------------------------</PRE></UL><P>Therefore if code size is an issue of any importance, unrolling any more than around 3 times is unlikely to pay off with regard to branch overhead elimination.<P><UL><LI>Combining LDR's and STR's into LDM and STM<P>The number of LDR's or STR's which can be combined into a single LDM or STM is restricted by the number of available registers.  In this instance 10 registers is the most which are likely to be usable.  This would result in unrolling the loop 10 times for the above example.  Another case to consider is unrolling 3 times, as this seems to be a good compromise for branch overhead reduction.</UL><UL><LI>Other optimisations<P>Upon examining the unrolled code below, it can be seen that it is only necessary to execute the MOV once per loop, thus saving another 2 IF cycles per loop for the 3 times unrolled code, and another 9 IF cycles per loop for the 10 times unrolled code.</UL><P>Here is the code unrolled three times and then optimised as described above:<P>    <CODE><PRE>    LDR     R2, [R0], #4                          ; Preload y[i]Loop    LDMIA   R0!, {R3-R5}                          ; Load y[i+1] to y[i+3]    SUB     R2, R2, R3                            ; x[i]   = y[i]   - y[i+1]    SUB     R3, R3, R4                            ; x[i+1] = y[i+1] - y[i+2]    SUB     R4, R4, R5                            ; x[i+2] = y[i+2] - y[i+3]    STMIA   R1!, {R2-R4}                          ; Store x[i] to x[i+2]    MOV     R2, R5                                ; y[i+3] is the next y[i]    CMP     R0, R6                                ; Finished ?    BLT     Loop</CODE></PRE>Analysing how this code executes for a y[] array of size 100, as described above for the unrolled code produces the following results:<P><CODE><PRE>339 IF (66 caused by branching), 101 R, 34 WB, 100 W (574 cycles)Code size: 9 instructionsSaving over unrolled code: 460 IF, 67 WB</CODE></PRE>Here is the code unrolled ten times and then optimised in the same way:<P>    <CODE><PRE>    LDR     R2, [R0], #4                               ; Preload y[i]Loop    LDMIA   R0!, {R3-R12}                              ; Load y[i+1] to y[i+10]    SUB     R2,  R2,  R3                               ; x[i]   = y[i]   - y[i+1]    SUB     R3,  R3,  R4                               ; x[i+1] = y[i+1] - y[i+2]    SUB     R4,  R4,  R5                               ; x[i+2] = y[i+2] - y[i+3]    SUB     R5,  R5,  R6                               ; x[i+3] = y[i+3] - y[i+4]    SUB     R6,  R6,  R7                               ; x[i+4] = y[i+4] - y[i+5]    SUB     R7,  R7,  R8                               ; x[i+5] = y[i+5] - y[i+6]    SUB     R8,  R8,  R9                               ; x[i+6] = y[i+6] - y[i+7]    SUB     R9,  R9,  R10                              ; x[i+7] = y[i+7] - y[i+8]    SUB     R10, R10, R11                              ; x[i+8] = y[i+8] - y[i+9]    SUB     R11, R11, R12                              ; x[i+9] = y[i+9] - y[i+10]    STMIA   R1!, {R2-R11}                              ; Store x[i] to x[i+9]    MOV     R2,  R12                                   ; y[i+10] is the next y[i]    CMP     R0,  R13                                   ; Finished ?    BLT     Loop</CODE></PRE>Analysing how this code executes for a y[] array of size 100, produces the following results:<P><CODE><PRE>169 IF (18 caused by branching), 101 R, 10 WB, 100 W (380 cycles)Code size: 16 instructionsSaving over unrolled code: 630 IF, 91 WB</CODE></PRE>Thus for this problem, unless the extra seven instructions make the code too large unrolling ten times is likely to be the optimum solution.<P>However, loop unrolling is not always a good idea, especially when the optimisation between one iteration and the next is small.  Consider the following loop which copies an area of memory:<P><CODE><PRE>Loop    LDMIA  R0!,{R3-R14}    STMIA  R1!,{R3-R14}    CMP    R0, #LimitAddress    BNE    Loop</CODE></PRE>This could be unrolled as follows:<P><CODE><PRE>Loop    LDMIA  R0!,{R3-R14}    STMIA  R1!,{R3-R14}    LDMIA  R0!,{R3-R14}    STMIA  R1!,{R3-R14}    LDMIA  R0!,{R3-R14}    STMIA  R1!,{R3-R14}    LDMIA  R0!,{R3-R14}    STMIA  R1!,{R3-R14}    CMP    R0, #LimitAddress    BLT    Loop</CODE></PRE>In this code the CMP and BNE will be executed only a quarter as often, but this will  give only a small saving.  However, other issues should be taken into account:<P><UL><LI>If in the above case the amount of data to be transferred was not a multiple of 48, then this amount of loop unrolling will copy too much data.  This may be catastrophic, or may merely be inefficient.<P><LI>On a cached ARM processor, the larger the inner loop, the more likely it is that the loop will not stay entirely in the cache.  In this case, it is not obvious at what point the performance gain due to unrolling is offset by the performance loss due to cache misses, or the disadvantage of larger code.<P><LI>On an ARM processor with a write buffer, the loop unrolling in the above example is unlikely to help.  If the data being copied is not in the cache, then  every LDMIA will be stalled while the write buffer empties.  Thus the time the CMP and BNE take is irrelevent, as the processor will be stalled on the following LDMIA.</UL>Loop unrolling can be a useful technique, but be warned that it doesn't always gain anything, and in some situations can reduce performance - detailed analysis is often necessary.<P><H3>Write buffer stalling</H3>On ARM processors with a write buffer, performance can be maximised by writing code which avoids stalling due to the write buffer.  For a write buffer with 2 tags and 8 words such as the ARM610, no three STR or STM instructions should be close together (as the third will be stalled until the first has finished).  Similarly no two STR or STM instructions which together store more than 8 words should be close together, as the second will be stalled until there is space in the write buffer.<P>Rearranging code so that the write buffer does not cause a stall in this way is often hard, but is frequently worth the effort, and in any case it is always wise to be aware of this performance factor.<P><H3>16-bit data</H3>If possible treat 16-bit data as 32-bit data.  However, if this is not possible, then be aware that it is possible to make use of the barrel shifter and non word-aligned LDR's in order to make working with 16-bit data more efficient than might be initially imagined.  See  for a full discussion of this topic.<P><H3>8-bit data</H3>When processing a sequence of byte sized objects (eg. strings), the number of loads and stores can be reduced if the data is loaded a word at a time and then processed a byte at a time by extracting the bytes using the barrel shifter.<P><H3>The floating point emulator</H3>If the software-only floating point emulator is being used then floating point instructions should placed sequentially, as the floating point emulator will detect that the next instruction is also a floating point instruction, and will emulate it without leaving the undefined instruction code.<P>Note that this advice is not applicable to systems which use the ARM FPA co-processor.<P><H3>Make full use of cache lines</H3>In order to help the cache on a cached ARM processor maintain a high hit rate for data, place frequently accessed data values together so that the are loaded into the same cache line, rather scattering them around memory, as this will require more cache lines to be loaded, and kept in the cache.<P>In a similar vein, commonly called subroutines (especially short ones) will often run more quickly on a cached ARM processor if the entry address is aligned so that it will be loaded into the first word of a cache line.  On the ARM610, for example this means quad-word aligned.  This ensures that all four words of the first line fetch will be subsequently used by instruction fetches before another line fetch is caused by an instruction fetch.  This technique is most useful for large programs which do not cache well, as the number of times the code will have to feteched from memory is not likely to be significant if the program does cache well.<P><H3>Minimise non-sequential cycles</H3>This technique is only appropriate to un-cached ARM processors, and is intended for memory systems in which non-sequential memory accesses take longer than sequential memory accesses.<P>Consider such a system in which the length of memory bursts is B.  That is, if executing a long sequence of data operations, the memory accesses which result are: one non-sequential memory cycle followed by B-1 sequential memory cycles.  eg. DRAM controlled by the ARM memory manager MEMC1a.<P>This sequence of memory accesses will be broken up by several ARM instruction types: Load or Store (single or multiple), Data Swap, Branch instructions, SWI's and other instructions which modify the PC.<P>By placing these instructions carefully, so that they break up the normal sequence of memory cycles only where a non-sequential cycle was about to occur anyway, the number of sequential cycles which are turned into longer non-sequential cycles can be minimised.<P>For a memory system in which has memory bursts of length B, the optimal position for instructions which break up the memory cycle sequence is 3 words before the next B-word boundary.<P>To help explain this consider a memory system with memory bursts of length 4 (ie. quad word bursts), the optimal position for these break-up instructions is 16-12=4 bytes from a quad-word offset.  To demonstrate this is the case imagine executing the following code in this system:<P><CODE><PRE>0x0000  Data Instr 10x0004  STR0x0008  Data Instr 20x000C  Data Instr 30x0010  Data Instr 4</CODE></PRE>The memory cycles executing this code will produce are (taking into account the ARM instruction pipeline):<P><CODE><PRE>Instruction Fetch 0x0000 (Non Seq)Instruction Fetch 0x0004 (Seq)Instruction Fetch 0x0008 (Seq)      + Execute Data Instr 1Instruction Fetch 0x000C (Seq)      + Execute STRData Write               (Non Seq)Instruction Fetch 0x0010 (Non Seq)  + Execute Data Instr 2Instruction Fetch 0x0014 (Seq)      + Execute Data Instr 3</CODE></PRE>The instruction fetch after the Data Write cycle had to be non-sequential cycle, but since the instruction fetch was of a quad-word aligned address it had to be non-sequential anyway.  Hence the STR is optimally positioned to avoid changing sequential instruction fetches into non-sequential instruction fetches.<P><H3>Use an efficient algorithm</H3>Despite all these techniques for optimising ARM Assembly Language, it is important that care is taken to start off with an efficient algorithm - all the optimisations in the world won't turn bubble sort into an 'n log n' sorting algorithm!<P><H2>Related topics</H2>Most of the recipes in this chapter are likely to have some relevance. Specific references have been indicated above for particular topics.<P><P>